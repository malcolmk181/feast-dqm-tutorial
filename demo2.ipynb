{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://gitlab.redchimney.com/api/v4/projects/5663/packages/pypi/simple\n",
      "Requirement already satisfied: feast[ge] in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (0.33.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.0.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (8.1.6)\n",
      "Requirement already satisfied: colorama<1,>=0.3.9 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (0.4.6)\n",
      "Requirement already satisfied: dill~=0.3.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (0.3.7)\n",
      "Requirement already satisfied: fastavro<2,>=1.1.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (1.8.2)\n",
      "Requirement already satisfied: grpcio<2,>=1.47.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (1.57.0)\n",
      "Requirement already satisfied: grpcio-reflection<2,>=1.47.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (1.57.0)\n",
      "Requirement already satisfied: Jinja2<4,>=2 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (3.1.2)\n",
      "Requirement already satisfied: jsonschema in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (4.19.0)\n",
      "Requirement already satisfied: mmh3 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (4.0.1)\n",
      "Requirement already satisfied: numpy<3,>=1.22 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (1.25.2)\n",
      "Requirement already satisfied: pandas<2,>=1.4.3 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (1.5.3)\n",
      "Requirement already satisfied: pandavro~=1.5.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (1.5.2)\n",
      "Requirement already satisfied: protobuf<5,>3.20 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (4.24.0)\n",
      "Requirement already satisfied: proto-plus<2,>=1.20.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (1.22.3)\n",
      "Requirement already satisfied: pyarrow<12,>=4 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (11.0.0)\n",
      "Requirement already satisfied: pydantic<2,>=1 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (1.10.12)\n",
      "Requirement already satisfied: pygments<3,>=2.12.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (2.16.1)\n",
      "Requirement already satisfied: PyYAML<7,>=5.4.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (2.31.0)\n",
      "Requirement already satisfied: SQLAlchemy[mypy]<2,>1 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (1.4.49)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (0.9.0)\n",
      "Requirement already satisfied: tenacity<9,>=7 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (8.2.3)\n",
      "Requirement already satisfied: toml<1,>=0.10.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (0.10.2)\n",
      "Requirement already satisfied: tqdm<5,>=4 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (4.66.1)\n",
      "Requirement already satisfied: typeguard==2.13.3 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (2.13.3)\n",
      "Requirement already satisfied: fastapi<1,>=0.68.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (0.99.1)\n",
      "Requirement already satisfied: uvicorn[standard]<1,>=0.14.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (0.23.2)\n",
      "Requirement already satisfied: gunicorn in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (21.2.0)\n",
      "Requirement already satisfied: dask>=2021.1.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (2023.8.0)\n",
      "Requirement already satisfied: bowler in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (0.9.0)\n",
      "Requirement already satisfied: httpx>=0.23.3 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (0.24.1)\n",
      "Requirement already satisfied: great-expectations<0.16.0,>=0.15.41 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from feast[ge]) (0.15.50)\n",
      "Requirement already satisfied: cloudpickle>=1.5.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from dask>=2021.1.0->feast[ge]) (2.2.1)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from dask>=2021.1.0->feast[ge]) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from dask>=2021.1.0->feast[ge]) (23.1)\n",
      "Requirement already satisfied: partd>=1.2.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from dask>=2021.1.0->feast[ge]) (1.4.0)\n",
      "Requirement already satisfied: toolz>=0.10.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from dask>=2021.1.0->feast[ge]) (0.12.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from dask>=2021.1.0->feast[ge]) (6.8.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from fastapi<1,>=0.68.0->feast[ge]) (0.27.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from fastapi<1,>=0.68.0->feast[ge]) (4.7.1)\n",
      "Requirement already satisfied: altair<4.2.1,>=4.0.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (4.2.0)\n",
      "Requirement already satisfied: cryptography>=3.2 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (41.0.3)\n",
      "Requirement already satisfied: Ipython>=7.16.3 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (8.14.0)\n",
      "Requirement already satisfied: ipywidgets>=7.5.1 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (8.1.0)\n",
      "Requirement already satisfied: jsonpatch>=1.22 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (1.33)\n",
      "Requirement already satisfied: makefun<2,>=1.7.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (1.15.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.7.1 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (3.20.1)\n",
      "Requirement already satisfied: mistune>=0.8.4 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (3.0.1)\n",
      "Requirement already satisfied: nbformat>=5.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (5.9.2)\n",
      "Requirement already satisfied: notebook>=6.4.10 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (7.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.4 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2021.3 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (2023.3)\n",
      "Requirement already satisfied: ruamel.yaml<0.17.18,>=0.16 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.17.17)\n",
      "Requirement already satisfied: scipy>=0.19.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (1.11.1)\n",
      "Requirement already satisfied: tzlocal>=1.2 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (5.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from great-expectations<0.16.0,>=0.15.41->feast[ge]) (1.26.16)\n",
      "Requirement already satisfied: certifi in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from httpx>=0.23.3->feast[ge]) (2023.7.22)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from httpx>=0.23.3->feast[ge]) (0.17.3)\n",
      "Requirement already satisfied: idna in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from httpx>=0.23.3->feast[ge]) (3.4)\n",
      "Requirement already satisfied: sniffio in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from httpx>=0.23.3->feast[ge]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from Jinja2<4,>=2->feast[ge]) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jsonschema->feast[ge]) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jsonschema->feast[ge]) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jsonschema->feast[ge]) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jsonschema->feast[ge]) (0.9.2)\n",
      "Requirement already satisfied: six>=1.9 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from pandavro~=1.5.0->feast[ge]) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from requests->feast[ge]) (3.2.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from SQLAlchemy[mypy]<2,>1->feast[ge]) (2.0.2)\n",
      "Requirement already satisfied: sqlalchemy2-stubs in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from SQLAlchemy[mypy]<2,>1->feast[ge]) (0.0.2a35)\n",
      "Requirement already satisfied: mypy>=0.910 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from SQLAlchemy[mypy]<2,>1->feast[ge]) (1.5.0)\n",
      "Requirement already satisfied: h11>=0.8 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from uvicorn[standard]<1,>=0.14.0->feast[ge]) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from uvicorn[standard]<1,>=0.14.0->feast[ge]) (0.6.0)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from uvicorn[standard]<1,>=0.14.0->feast[ge]) (1.0.0)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from uvicorn[standard]<1,>=0.14.0->feast[ge]) (0.17.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from uvicorn[standard]<1,>=0.14.0->feast[ge]) (0.19.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from uvicorn[standard]<1,>=0.14.0->feast[ge]) (11.0.3)\n",
      "Requirement already satisfied: fissix in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from bowler->feast[ge]) (21.11.13)\n",
      "Requirement already satisfied: moreorless>=0.2.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from bowler->feast[ge]) (0.4.0)\n",
      "Requirement already satisfied: volatile in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from bowler->feast[ge]) (2.1.0)\n",
      "Requirement already satisfied: entrypoints in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from altair<4.2.1,>=4.0.0->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from cryptography>=3.2->great-expectations<0.16.0,>=0.15.41->feast[ge]) (1.15.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx>=0.23.3->feast[ge]) (3.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from importlib-metadata>=4.13.0->dask>=2021.1.0->feast[ge]) (3.16.2)\n",
      "Requirement already satisfied: backcall in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.2.0)\n",
      "Requirement already satisfied: decorator in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.19.0)\n",
      "Requirement already satisfied: matplotlib-inline in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (3.0.39)\n",
      "Requirement already satisfied: stack-data in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (5.9.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (4.8.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from ipywidgets>=7.5.1->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.1.4)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.7 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from ipywidgets>=7.5.1->great-expectations<0.16.0,>=0.15.41->feast[ge]) (4.0.8)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from ipywidgets>=7.5.1->great-expectations<0.16.0,>=0.15.41->feast[ge]) (3.0.8)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jsonpatch>=1.22->great-expectations<0.16.0,>=0.15.41->feast[ge]) (2.4)\n",
      "Requirement already satisfied: mypy-extensions>=1.0.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from mypy>=0.910->SQLAlchemy[mypy]<2,>1->feast[ge]) (1.0.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from mypy>=0.910->SQLAlchemy[mypy]<2,>1->feast[ge]) (2.0.1)\n",
      "Requirement already satisfied: fastjsonschema in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from nbformat>=5.0->great-expectations<0.16.0,>=0.15.41->feast[ge]) (2.18.0)\n",
      "Requirement already satisfied: jupyter-core in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from nbformat>=5.0->great-expectations<0.16.0,>=0.15.41->feast[ge]) (5.3.1)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (2.7.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (2.24.0)\n",
      "Requirement already satisfied: jupyterlab<5,>=4.0.2 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (4.0.5)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.2.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (6.3.3)\n",
      "Requirement already satisfied: locket in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from partd>=1.2.0->dask>=2021.1.0->feast[ge]) (1.0.0)\n",
      "Requirement already satisfied: appdirs in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from fissix->bowler->feast[ge]) (1.4.4)\n",
      "Requirement already satisfied: exceptiongroup in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx>=0.23.3->feast[ge]) (1.1.3)\n",
      "Requirement already satisfied: pycparser in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.2->great-expectations<0.16.0,>=0.15.41->feast[ge]) (2.21)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jedi>=0.16->Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.8.3)\n",
      "Requirement already satisfied: argon2-cffi in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (23.1.0)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (8.3.0)\n",
      "Requirement already satisfied: jupyter-events>=0.6.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.7.0)\n",
      "Requirement already satisfied: jupyter-server-terminals in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (7.7.3)\n",
      "Requirement already satisfied: overrides in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.17.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (25.1.1)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.17.1)\n",
      "Requirement already satisfied: websocket-client in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (1.6.1)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-core->nbformat>=5.0->great-expectations<0.16.0,>=0.15.41->feast[ge]) (3.10.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (2.0.4)\n",
      "Requirement already satisfied: ipykernel in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (6.25.1)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyterlab<5,>=4.0.2->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (2.12.1)\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.9.14)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from pexpect>4.3->Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from stack-data->Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from stack-data->Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from stack-data->Ipython>=7.16.3->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.2.2)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (2.0.7)\n",
      "Requirement already satisfied: rfc3339-validator in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jupyter-events>=0.6.0->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (4.12.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (6.0.0)\n",
      "Requirement already satisfied: defusedxml in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.2.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (1.2.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from argon2-cffi->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (21.2.0)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (1.6.7.post1)\n",
      "Requirement already satisfied: nest-asyncio in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (1.5.7)\n",
      "Requirement already satisfied: psutil in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from ipykernel->jupyterlab<5,>=4.0.2->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (5.9.5)\n",
      "Requirement already satisfied: webencodings in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (0.5.1)\n",
      "Requirement already satisfied: fqdn in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jsonschema->feast[ge]) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jsonschema->feast[ge]) (20.11.0)\n",
      "Requirement already satisfied: uri-template in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jsonschema->feast[ge]) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from jsonschema->feast[ge]) (1.13)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=6.4.10->great-expectations<0.16.0,>=0.15.41->feast[ge]) (2.4.1)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/qs/.conda/envs/workspace/lib/python3.10/site-packages (from isoduration->jsonschema->feast[ge]) (1.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install feast[ge]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet\n",
    "import pandas as pd\n",
    "\n",
    "from feast import FeatureView, Entity, FeatureStore, FeatureService, Field, BatchFeatureView\n",
    "from feast.types import Float64, Int64\n",
    "from feast.value_type import ValueType\n",
    "from feast.data_format import ParquetFormat\n",
    "from feast.on_demand_feature_view import on_demand_feature_view\n",
    "from feast.infra.offline_stores.file_source import FileSource\n",
    "from feast.infra.offline_stores.file import SavedDatasetFileStorage\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from feast.dqm.profilers.ge_profiler import ge_profiler\n",
    "\n",
    "from great_expectations.core.expectation_suite import ExpectationSuite\n",
    "from great_expectations.dataset import PandasDataset\n",
    "\n",
    "from feast.dqm.errors import ValidationFailed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation with Feast Saved Datasets & Great Expectations\n",
    "\n",
    "The purpose of this notebook is to showcase the potential for data validation with Feast's Saved Dataset feature, harnessing the power of the Great Expectations validation engine.\n",
    "\n",
    "## The Data\n",
    "\n",
    "### trips_stats.parquet\n",
    "\n",
    "Driver statistics for a taxi service. Each column corresponds to a driver's statistics for the day, given that they had at least one trip. The columns are: `'taxi_id', 'day', 'total_miles_travelled', 'total_trip_seconds', 'total_earned', 'trip_count'`\n",
    "\n",
    "### trips_stats_featurized.parquet\n",
    "\n",
    "This is my attempt at featurizing the trip stats data into something a logistic regression model could use. Please bear with me, I have zero data science experience, so these features are arbitrarily chosen without regard and the code to get there (in demo.ipynb) is NOT ideal lol. Anyway, the columns are: `'taxi_id', 'day', 'trip_count_1_to_10', 'trip_count_11_to_25', 'trip_count_26_to_50', 'trip_count_51_to_100', 'trip_count_101_or_higher', 'total_miles_0_to_10', 'total_miles_10_to_25', 'total_miles_25_to_50', 'total_miles_50_to_100', 'total_miles_100_to_250', 'total_miles_250_to_500', 'total_miles_500_to_1000', 'total_miles_1000_or_higher', 'seconds_0_3600', 'seconds_3600_to_7200', 'seconds_7200_to_14400', 'seconds_14400_to_28800', 'seconds_28800_to_43200', 'seconds_43200_or_higher', 'earned_0_to_50', 'earned_50_to_100', 'earned_100_to_250', 'earned_250_to_500', 'earned_500_to_1000', 'earned_1000_or_higher'`\n",
    "\n",
    "## The Goal\n",
    "\n",
    "The goal is to show how Feast & Great Expectations can be used to:\n",
    "\n",
    "1. Validate the structure and content of pre-featurization data. This will break the assumption that Feast stores only feature data, but it will show the power of Great Expectations.\n",
    "2. Track the drift of distributions of features over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Validating the schema of pre-featurization data\n",
    "\n",
    "I am first going to initialize a Feast feature store using the `feature_store.yaml` config file.\n",
    "\n",
    "Then I am going to load the `trips_stats.parquet` file, and register it as `FileSource` for Feast.\n",
    "\n",
    "`taxi_id` is the identifier for different drivers, so I will register an Entity for the `taxi_id`s.\n",
    "\n",
    "I'll also make a feature view that corresponds with the columns of this table. This isn't featurized data yet, but it is needed in the Feast context. Great Expectations (when configured outside of Feast) does not need this, but I will do so for ease of demo.\n",
    "\n",
    "I then apply these to the store object, saving these objects' metadata in the Feast Registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "store = FeatureStore(repo_path=\".\", fs_yaml_file=\"feature_store.yaml\")\n",
    "\n",
    "raw_data_source = FileSource(\n",
    "    path=\"trips_stats.parquet\",\n",
    "    name=\"raw_data\",\n",
    "    file_format=ParquetFormat(),\n",
    "    description=\"Raw trip stat data - not featurized.\",\n",
    "    owner=\"Malcolm Keyes\",\n",
    "    timestamp_field=\"day\"\n",
    ")\n",
    "\n",
    "taxi_entity = Entity(\n",
    "    name='taxi_id',\n",
    "    join_keys=['taxi_id'],\n",
    "    value_type=ValueType.STRING,\n",
    "    description=\"The ID unique to every driver.\",\n",
    "    owner=\"Malcolm Keyes\"\n",
    ")\n",
    "\n",
    "raw_data_fv = FeatureView(\n",
    "    name=\"raw_data_fv\",\n",
    "    source= raw_data_source,\n",
    "    entities=[taxi_entity],\n",
    "    description=\"The columns of the raw data.\",\n",
    "    owner=\"Malcolm Keyes\"\n",
    ")\n",
    "\n",
    "store.apply([raw_data_source, taxi_entity, raw_data_fv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FeatureView(name = raw_data_fv, entities = ['taxi_id'], ttl = 0:00:00, stream_source = None, batch_source = {\n",
       "  \"type\": \"BATCH_FILE\",\n",
       "  \"timestampField\": \"day\",\n",
       "  \"fileOptions\": {\n",
       "    \"fileFormat\": {\n",
       "      \"parquetFormat\": {}\n",
       "    },\n",
       "    \"uri\": \"trips_stats.parquet\"\n",
       "  },\n",
       "  \"name\": \"raw_data\",\n",
       "  \"description\": \"Raw trip stat data - not featurized.\",\n",
       "  \"owner\": \"Malcolm Keyes\"\n",
       "}, entity_columns = [taxi_id-String], features = [total_miles_travelled-Float64, total_trip_seconds-Int64, total_earned-Float64, trip_count-Int64], description = The columns of the raw data., tags = {}, owner = Malcolm Keyes, projection = FeatureViewProjection(name='raw_data_fv', name_alias=None, desired_features=[], features=[total_miles_travelled-Float64, total_trip_seconds-Int64, total_earned-Float64, trip_count-Int64], join_key_map={}), created_timestamp = 2023-08-11 19:39:45.382344, last_updated_timestamp = 2023-08-11 19:39:45.382344, online = True, materialization_intervals = [])>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store.get_feature_view(\"raw_data_fv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have registered registered the raw data source to the Feast Registry, along with the `taxi_id` entity, and a `raw_data` feature view.\n",
    "\n",
    "The data in the `trip_stats.parquet` file spans from January 1st, 2019 to December 31st, 2020. Before we do our data cleaning and featurization, lets create a suite of Expectations to describe the expected schema of the data.\n",
    "\n",
    "This will be a little hacky, but we're going to set aside a chunk of the data, from Feb 1, 2019 to Mar 1, 2019, and save it as a dataset. Then we will run a profiler on it and create a profile. Then we will save the dataset and profile together, as a reference dataset. Once this is done, then we can use this reference dataset to validate a `RetrievalJob` (the result of a `get_historical_features()` call) on some other chunk of the data - like the month of July 2019, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SavedDataset(name = february_saved_dataset, features = ['raw_data_fv:total_miles_travelled', 'raw_data_fv:total_trip_seconds', 'raw_data_fv:total_earned', 'raw_data_fv:trip_count'], join_keys = ['taxi_id'], storage = <feast.infra.offline_stores.file_source.SavedDatasetFileStorage object at 0x1376ba080>, full_feature_names = False, tags = {}, feature_service_name = None, _retrieval_job = <feast.infra.offline_stores.file.FileRetrievalJob object at 0x138018490>, min_event_timestamp = 2019-02-01 00:00:00+00:00, max_event_timestamp = 2019-03-01 00:00:00+00:00, created_timestamp = 2023-08-11 20:30:38.170170, last_updated_timestamp = 2023-08-11 20:30:38.170170)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's generate an entity dataframe with the timestamps we want,\n",
    "# and the entities we want (in this case, all of them)\n",
    "\n",
    "timestamps = pd.DataFrame()\n",
    "timestamps[\"event_timestamp\"] = pd.date_range(\"2019-02-01\", \"2019-03-01\", freq=\"D\")\n",
    "\n",
    "# an array of the taxi_ids has already been saved to entities.parquet\n",
    "taxi_ids = pyarrow.parquet.read_table(\"entities.parquet\").to_pandas()\n",
    "\n",
    "# Cross merge (aka relation multiplication) produces entity dataframe with each taxi_id repeated for each timestamp:\n",
    "entity_df = pd.merge(taxi_ids, timestamps, how='cross')\n",
    "\n",
    "# retrieve the data\n",
    "february_job = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[ # could have also made a feature service here instead of listing individual features\n",
    "        \"raw_data_fv:total_miles_travelled\",\n",
    "        \"raw_data_fv:total_trip_seconds\",\n",
    "        \"raw_data_fv:total_earned\",\n",
    "        \"raw_data_fv:trip_count\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "# turn this retrieved data into a saved dataset, persisting this query\n",
    "# to a file and the metadata to the feast registry\n",
    "store.create_saved_dataset(\n",
    "    from_=february_job,\n",
    "    name=\"february_saved_dataset\",\n",
    "    storage=SavedDatasetFileStorage(path=\"february_saved_ds.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our saved dataset, it's time to create a profile of Great Expectations \"Expectations\". Each Expectation is an assertion about the data.\n",
    "\n",
    "There is a gallery of all the different expectations (there are over 300) here: [Great Expectations - Expectation Gallery](https://greatexpectations.io/expectations/?filterType=Backend+support&viewType=Summary&showFilters=true&subFilterValues=pandas)\n",
    "\n",
    "Let's make sure that the data meets the structure we expect.\n",
    "\n",
    "The following columns should be present and have the following datatypes:\n",
    "\n",
    "- `total_miles_travelled` : `float64`\n",
    "- `total_trip_seconds` : `int64`\n",
    "- `total_earned` : `float64`\n",
    "- `trip_count` : `int64`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ge_profiler\n",
    "def structure_profiler(ds: PandasDataset) -> ExpectationSuite:\n",
    "\n",
    "    ds.expect_column_to_exist(\n",
    "        column=\"total_miles_travelled\"\n",
    "    )\n",
    "\n",
    "    ds.expect_column_values_to_be_of_type(\n",
    "        column=\"total_miles_travelled\",\n",
    "        type_=\"float64\"\n",
    "    )\n",
    "\n",
    "    ds.expect_column_to_exist(\n",
    "        column=\"total_trip_seconds\"\n",
    "    )\n",
    "\n",
    "    ds.expect_column_values_to_be_of_type(\n",
    "        column=\"total_trip_seconds\",\n",
    "        type_=\"int64\"\n",
    "    )\n",
    "\n",
    "    ds.expect_column_to_exist(\n",
    "        column=\"total_earned\"\n",
    "    )\n",
    "\n",
    "    ds.expect_column_values_to_be_of_type(\n",
    "        column=\"total_earned\",\n",
    "        type_=\"float64\"\n",
    "    )\n",
    "\n",
    "    ds.expect_column_to_exist(\n",
    "        column=\"trip_count\"\n",
    "    )\n",
    "\n",
    "    ds.expect_column_values_to_be_of_type(\n",
    "        column=\"trip_count\",\n",
    "        type_=\"int64\"\n",
    "    )\n",
    "\n",
    "    return ds.get_expectation_suite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<GEProfile with expectations: [\n",
       "  {\n",
       "    \"expectation_type\": \"expect_column_to_exist\",\n",
       "    \"kwargs\": {\n",
       "      \"column\": \"total_miles_travelled\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  {\n",
       "    \"expectation_type\": \"expect_column_values_to_be_of_type\",\n",
       "    \"kwargs\": {\n",
       "      \"column\": \"total_miles_travelled\",\n",
       "      \"type_\": \"float64\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  {\n",
       "    \"expectation_type\": \"expect_column_to_exist\",\n",
       "    \"kwargs\": {\n",
       "      \"column\": \"total_trip_seconds\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  {\n",
       "    \"expectation_type\": \"expect_column_values_to_be_of_type\",\n",
       "    \"kwargs\": {\n",
       "      \"column\": \"total_trip_seconds\",\n",
       "      \"type_\": \"int64\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  {\n",
       "    \"expectation_type\": \"expect_column_to_exist\",\n",
       "    \"kwargs\": {\n",
       "      \"column\": \"total_earned\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  {\n",
       "    \"expectation_type\": \"expect_column_values_to_be_of_type\",\n",
       "    \"kwargs\": {\n",
       "      \"column\": \"total_earned\",\n",
       "      \"type_\": \"float64\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  {\n",
       "    \"expectation_type\": \"expect_column_to_exist\",\n",
       "    \"kwargs\": {\n",
       "      \"column\": \"trip_count\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  },\n",
       "  {\n",
       "    \"expectation_type\": \"expect_column_values_to_be_of_type\",\n",
       "    \"kwargs\": {\n",
       "      \"column\": \"trip_count\",\n",
       "      \"type_\": \"int64\"\n",
       "    },\n",
       "    \"meta\": {}\n",
       "  }\n",
       "]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load saved dataset from earlier and show contents\n",
    "ds = store.get_saved_dataset('february_saved_dataset')\n",
    "ds.to_df()\n",
    "\n",
    "# Now let's print out the profile that will be generated - this should match\n",
    "# the expectations we included\n",
    "\n",
    "profile = ds.get_profile(profiler=structure_profiler)\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This output isn't super interesting by itself - mostly because this profiler didn't pull any information from the dataset. But - stick with me - the next one will pull values from the characteristics of the dataset, which will be used to generate values for quantitative expectations.\n",
    "\n",
    "Let's register this dataset and profile as a reference dataset in feast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "structural_validation_reference = ds.as_reference(\n",
    "    name=\"structural_validation_reference_dataset\",\n",
    "    profiler=structure_profiler\n",
    ")\n",
    "\n",
    "store.apply([structural_validation_reference])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's pull down the data for July 2019, and see that it meets our structural expectations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = pd.DataFrame()\n",
    "timestamps[\"event_timestamp\"] = pd.date_range(\"2019-07-01\", \"2019-08-01\", freq=\"D\")\n",
    "\n",
    "# an array of the taxi_ids has already been saved to entities.parquet\n",
    "taxi_ids = pyarrow.parquet.read_table(\"entities.parquet\").to_pandas()\n",
    "\n",
    "# Cross merge (aka relation multiplication) produces entity dataframe with each taxi_id repeated for each timestamp:\n",
    "entity_df = pd.merge(taxi_ids, timestamps, how='cross')\n",
    "\n",
    "# retrieve the data\n",
    "july_job = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[ # could have also made a feature service here instead of listing individual features\n",
    "        \"raw_data_fv:total_miles_travelled\",\n",
    "        \"raw_data_fv:total_trip_seconds\",\n",
    "        \"raw_data_fv:total_earned\",\n",
    "        \"raw_data_fv:trip_count\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>total_miles_travelled</th>\n",
       "      <th>total_trip_seconds</th>\n",
       "      <th>total_earned</th>\n",
       "      <th>trip_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f58be545baf2a44a9575194c9d6d8276e32781fae982ac...</td>\n",
       "      <td>2019-08-01 00:00:00+00:00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5820</td>\n",
       "      <td>103.25</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f58be545baf2a44a9575194c9d6d8276e32781fae982ac...</td>\n",
       "      <td>2019-07-31 00:00:00+00:00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5820</td>\n",
       "      <td>103.25</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f58be545baf2a44a9575194c9d6d8276e32781fae982ac...</td>\n",
       "      <td>2019-07-30 00:00:00+00:00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5820</td>\n",
       "      <td>103.25</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f58be545baf2a44a9575194c9d6d8276e32781fae982ac...</td>\n",
       "      <td>2019-07-29 00:00:00+00:00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5820</td>\n",
       "      <td>103.25</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f58be545baf2a44a9575194c9d6d8276e32781fae982ac...</td>\n",
       "      <td>2019-07-28 00:00:00+00:00</td>\n",
       "      <td>26.0</td>\n",
       "      <td>5820</td>\n",
       "      <td>103.25</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             taxi_id  \\\n",
       "0  f58be545baf2a44a9575194c9d6d8276e32781fae982ac...   \n",
       "1  f58be545baf2a44a9575194c9d6d8276e32781fae982ac...   \n",
       "2  f58be545baf2a44a9575194c9d6d8276e32781fae982ac...   \n",
       "3  f58be545baf2a44a9575194c9d6d8276e32781fae982ac...   \n",
       "4  f58be545baf2a44a9575194c9d6d8276e32781fae982ac...   \n",
       "\n",
       "            event_timestamp  total_miles_travelled  total_trip_seconds  \\\n",
       "0 2019-08-01 00:00:00+00:00                   26.0                5820   \n",
       "1 2019-07-31 00:00:00+00:00                   26.0                5820   \n",
       "2 2019-07-30 00:00:00+00:00                   26.0                5820   \n",
       "3 2019-07-29 00:00:00+00:00                   26.0                5820   \n",
       "4 2019-07-28 00:00:00+00:00                   26.0                5820   \n",
       "\n",
       "   total_earned  trip_count  \n",
       "0        103.25          11  \n",
       "1        103.25          11  \n",
       "2        103.25          11  \n",
       "3        103.25          11  \n",
       "4        103.25          11  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "july = july_job.to_df(\n",
    "    validation_reference=structural_validation_reference\n",
    ")\n",
    "july.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Feast pulls the data into a dataframe, it uses our reference to validate the new data against our expectations.\n",
    "\n",
    "Since we didn't encounter any exceptions, we know our expectations were met - the columns we specified are present and the data types are as expected!\n",
    "\n",
    "Now let's break a subset of the July data and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's pull from the actual data source - not what Feast returned\n",
    "\n",
    "trip_stats_parquet = pd.read_parquet(\"trips_stats.parquet\")\n",
    "\n",
    "july_subset = trip_stats_parquet.loc[trip_stats_parquet['day'].isin([\"2019-07-01 00:00:00+00:00\"])].head(5)\n",
    "\n",
    "july_subset.rename(\n",
    "    columns={\n",
    "        \"total_miles_travelled\":\"total_miles\",\n",
    "        \"event_timestamp\":\"day\"\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# and also change the datatype of a column\n",
    "july_subset = july_subset.astype({\"total_earned\":str})\n",
    "\n",
    "# then save as a parquet\n",
    "july_subset.to_parquet(\n",
    "    path=\"broken_july.parquet\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's pull it back in as a saved dataset\n",
    "# ... which means we need to set it up as a FileSource\n",
    "# and add a FeatureView, which breaks our Feast assumptions.\n",
    "\n",
    "broken_july_raw_data_source = FileSource(\n",
    "    path=\"broken_july.parquet\",\n",
    "    name=\"broken_july_raw_data\",\n",
    "    file_format=ParquetFormat(),\n",
    "    description=\"(Broken) raw trip stat data for July - not featurized.\",\n",
    "    owner=\"Malcolm Keyes\",\n",
    "    timestamp_field=\"day\"\n",
    ")\n",
    "\n",
    "broken_july_raw_data_fv = FeatureView(\n",
    "    name=\"broken_july_raw_data_fv\",\n",
    "    source= broken_july_raw_data_source,\n",
    "    entities=[taxi_entity],\n",
    "    description=\"The columns of the raw data.\",\n",
    "    owner=\"Malcolm Keyes\"\n",
    ")\n",
    "\n",
    "store.apply([broken_july_raw_data_source, broken_july_raw_data_fv])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Feature total_miles_travelled not found in projection broken_july_raw_data_fv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/workspace/lib/python3.10/site-packages/feast/feature_view_projection.py:77\u001b[0m, in \u001b[0;36mFeatureViewProjection.get_feature\u001b[0;34m(self, feature_name)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(field \u001b[39mfor\u001b[39;49;00m field \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures \u001b[39mif\u001b[39;49;00m field\u001b[39m.\u001b[39;49mname \u001b[39m==\u001b[39;49m feature_name)\n\u001b[1;32m     78\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m# Cross merge (aka relation multiplication) produces entity dataframe with each taxi_id repeated for each timestamp:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m entity_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(taxi_ids_subset, july_first_timestamps, how\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcross\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 17\u001b[0m broken_july_job \u001b[39m=\u001b[39m store\u001b[39m.\u001b[39;49mget_historical_features(\n\u001b[1;32m     18\u001b[0m     entity_df\u001b[39m=\u001b[39;49mentity_df,\n\u001b[1;32m     19\u001b[0m     features\u001b[39m=\u001b[39;49m[ \u001b[39m# could have also made a feature service here instead of listing individual features\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mbroken_july_raw_data_fv:total_miles_travelled\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     21\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mbroken_july_raw_data_fv:total_trip_seconds\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mbroken_july_raw_data_fv:total_earned\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     23\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mbroken_july_raw_data_fv:trip_count\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     24\u001b[0m     ]\n\u001b[1;32m     25\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/workspace/lib/python3.10/site-packages/feast/usage.py:299\u001b[0m, in \u001b[0;36mlog_exceptions_and_usage.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m     ctx\u001b[39m.\u001b[39mtraceback \u001b[39m=\u001b[39m _trace_to_log(traceback)\n\u001b[1;32m    298\u001b[0m     \u001b[39mif\u001b[39;00m traceback:\n\u001b[0;32m--> 299\u001b[0m         \u001b[39mraise\u001b[39;00m exc\u001b[39m.\u001b[39mwith_traceback(traceback)\n\u001b[1;32m    301\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    302\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/workspace/lib/python3.10/site-packages/feast/usage.py:288\u001b[0m, in \u001b[0;36mlog_exceptions_and_usage.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m ctx\u001b[39m.\u001b[39mattributes\u001b[39m.\u001b[39mupdate(attrs)\n\u001b[1;32m    287\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 288\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    289\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[39mif\u001b[39;00m ctx\u001b[39m.\u001b[39mexception:\n\u001b[1;32m    291\u001b[0m         \u001b[39m# exception was already recorded\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/workspace/lib/python3.10/site-packages/feast/feature_store.py:1095\u001b[0m, in \u001b[0;36mFeatureStore.get_historical_features\u001b[0;34m(self, entity_df, features, full_feature_names)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1087\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mRequest feature view is deprecated. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1088\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease use request data source instead\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1089\u001b[0m         \u001b[39mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m   1090\u001b[0m     )\n\u001b[1;32m   1092\u001b[0m \u001b[39m# TODO(achal): _group_feature_refs returns the on demand feature views, but it's not passed into the provider.\u001b[39;00m\n\u001b[1;32m   1093\u001b[0m \u001b[39m# This is a weird interface quirk - we should revisit the `get_historical_features` to\u001b[39;00m\n\u001b[1;32m   1094\u001b[0m \u001b[39m# pass in the on demand feature views as well.\u001b[39;00m\n\u001b[0;32m-> 1095\u001b[0m fvs, odfvs, request_fvs, request_fv_refs \u001b[39m=\u001b[39m _group_feature_refs(\n\u001b[1;32m   1096\u001b[0m     _feature_refs,\n\u001b[1;32m   1097\u001b[0m     all_feature_views,\n\u001b[1;32m   1098\u001b[0m     all_request_feature_views,\n\u001b[1;32m   1099\u001b[0m     all_on_demand_feature_views,\n\u001b[1;32m   1100\u001b[0m )\n\u001b[1;32m   1101\u001b[0m feature_views \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(view \u001b[39mfor\u001b[39;00m view, _ \u001b[39min\u001b[39;00m fvs)\n\u001b[1;32m   1102\u001b[0m on_demand_feature_views \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(view \u001b[39mfor\u001b[39;00m view, _ \u001b[39min\u001b[39;00m odfvs)\n",
      "File \u001b[0;32m~/.conda/envs/workspace/lib/python3.10/site-packages/feast/feature_store.py:2474\u001b[0m, in \u001b[0;36m_group_feature_refs\u001b[0;34m(features, all_feature_views, all_request_feature_views, all_on_demand_feature_views)\u001b[0m\n\u001b[1;32m   2472\u001b[0m view_name, feat_name \u001b[39m=\u001b[39m ref\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   2473\u001b[0m \u001b[39mif\u001b[39;00m view_name \u001b[39min\u001b[39;00m view_index:\n\u001b[0;32m-> 2474\u001b[0m     view_index[view_name]\u001b[39m.\u001b[39;49mprojection\u001b[39m.\u001b[39;49mget_feature(feat_name)  \u001b[39m# For validation\u001b[39;00m\n\u001b[1;32m   2475\u001b[0m     views_features[view_name]\u001b[39m.\u001b[39madd(feat_name)\n\u001b[1;32m   2476\u001b[0m \u001b[39melif\u001b[39;00m view_name \u001b[39min\u001b[39;00m on_demand_view_index:\n",
      "File \u001b[0;32m~/.conda/envs/workspace/lib/python3.10/site-packages/feast/feature_view_projection.py:79\u001b[0m, in \u001b[0;36mFeatureViewProjection.get_feature\u001b[0;34m(self, feature_name)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39m(field \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures \u001b[39mif\u001b[39;00m field\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m feature_name)\n\u001b[1;32m     78\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFeature \u001b[39m\u001b[39m{\u001b[39;00mfeature_name\u001b[39m}\u001b[39;00m\u001b[39m not found in projection \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname_to_use()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m     )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Feature total_miles_travelled not found in projection broken_july_raw_data_fv'"
     ]
    }
   ],
   "source": [
    "# Now let's try to pull the data and validate it\n",
    "# We'll use the previously made entity dataframe\n",
    "\n",
    "july_first_timestamps = pd.DataFrame()\n",
    "july_first_timestamps[\"event_timestamp\"] = pd.date_range(\"2019-07-01\", \"2019-07-01\", freq=\"D\")\n",
    "\n",
    "taxi_ids_subset = pd.DataFrame()\n",
    "taxi_ids_subset[\"taxi_id\"] = ['3d5fcccd2f2e4fb12eba0d94f8cdf657b20d405fece7c8a26571a7a0db7d8f8cf4f257a86ad49ad864417c6d8ceb0763b2207fbb57a78950f2566409e1b93fcd',\n",
    " 'd133de68d7dfb2069cd26b91cc2e0a934b2e3d125f7ce76dcbe456015776df5e688727e0abdd8b0016cdff2072ef3f65f6e10b7694560473c0bea15f75d1d562',\n",
    " '617d878f0486c82431cd73a6c1cee0992947dea1140244ca6e4ad1e1e167c0e7f1f8437e3b03c2a0ba0fd5b3f3eed869ee7e1ba9c3e0afe008e819cd4aa477d4',\n",
    " 'c531f081cad817a366cbae2254ce7a3bb370394b3b0f1399dfc8781333c3565ca188035bbff4d1d7652181b79712c103d01ab168cb6ca2580f2da4b324cb00a5',\n",
    " '59cfe5aef1ecdd4418c373fe7848fb1f9defcbcfe70f17f70bf11f05711c380494e26d4f4a1b9e1c1d873a2f741318864d34f23735ee3b20f07fc10af300d66a']\n",
    "\n",
    "# Cross merge (aka relation multiplication) produces entity dataframe with each taxi_id repeated for each timestamp:\n",
    "entity_df = pd.merge(taxi_ids_subset, july_first_timestamps, how='cross')\n",
    "\n",
    "broken_july_job = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[ # could have also made a feature service here instead of listing individual features\n",
    "        \"broken_july_raw_data_fv:total_miles_travelled\",\n",
    "        \"broken_july_raw_data_fv:total_trip_seconds\",\n",
    "        \"broken_july_raw_data_fv:total_earned\",\n",
    "        \"broken_july_raw_data_fv:trip_count\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, awesome! Feast caught that the features we requested didn't match the columns found in our broken table. We can check that box off for Feast.\n",
    "\n",
    "Let's try again, passing in the correct features this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try to pull the data and validate it\n",
    "# We'll use the previously made entity dataframe\n",
    "\n",
    "broken_july_job = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[ # could have also made a feature service here instead of listing individual features\n",
    "        \"broken_july_raw_data_fv:total_miles\", # this is the name we changed\n",
    "        \"broken_july_raw_data_fv:total_trip_seconds\",\n",
    "        \"broken_july_raw_data_fv:total_earned\",\n",
    "        \"broken_july_raw_data_fv:trip_count\"\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was successful! Now let's try to pull it into a Pandas dataframe, and verify it with our Validations we made earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"success\": false,\n",
      "    \"expectation_config\": {\n",
      "      \"expectation_type\": \"expect_column_to_exist\",\n",
      "      \"kwargs\": {\n",
      "        \"column\": \"total_miles_travelled\",\n",
      "        \"result_format\": \"COMPLETE\"\n",
      "      },\n",
      "      \"meta\": {}\n",
      "    },\n",
      "    \"result\": {},\n",
      "    \"meta\": {},\n",
      "    \"exception_info\": {\n",
      "      \"raised_exception\": false,\n",
      "      \"exception_message\": null,\n",
      "      \"exception_traceback\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"success\": false,\n",
      "    \"expectation_config\": {\n",
      "      \"expectation_type\": \"expect_column_values_to_be_of_type\",\n",
      "      \"kwargs\": {\n",
      "        \"column\": \"total_miles_travelled\",\n",
      "        \"type_\": \"float64\",\n",
      "        \"result_format\": \"COMPLETE\"\n",
      "      },\n",
      "      \"meta\": {}\n",
      "    },\n",
      "    \"result\": {},\n",
      "    \"meta\": {},\n",
      "    \"exception_info\": {\n",
      "      \"raised_exception\": true,\n",
      "      \"exception_traceback\": \"Traceback (most recent call last):\\n  File \\\"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py\\\", line 3802, in get_loc\\n    return self._engine.get_loc(casted_key)\\n  File \\\"pandas/_libs/index.pyx\\\", line 138, in pandas._libs.index.IndexEngine.get_loc\\n  File \\\"pandas/_libs/index.pyx\\\", line 165, in pandas._libs.index.IndexEngine.get_loc\\n  File \\\"pandas/_libs/hashtable_class_helper.pxi\\\", line 5745, in pandas._libs.hashtable.PyObjectHashTable.get_item\\n  File \\\"pandas/_libs/hashtable_class_helper.pxi\\\", line 5753, in pandas._libs.hashtable.PyObjectHashTable.get_item\\nKeyError: 'total_miles_travelled'\\n\\nThe above exception was the direct cause of the following exception:\\n\\nTraceback (most recent call last):\\n  File \\\"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/great_expectations/data_asset/data_asset.py\\\", line 949, in validate\\n    result = expectation_method(\\n  File \\\"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/great_expectations/data_asset/util.py\\\", line 76, in f\\n    return self.mthd(obj, *args, **kwargs)\\n  File \\\"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/great_expectations/dataset/pandas_dataset.py\\\", line 721, in expect_column_values_to_be_of_type\\n    self[column].dtype != \\\"object\\\"\\n  File \\\"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py\\\", line 3807, in __getitem__\\n    indexer = self.columns.get_loc(key)\\n  File \\\"/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py\\\", line 3804, in get_loc\\n    raise KeyError(key) from err\\nKeyError: 'total_miles_travelled'\\n\",\n",
      "      \"exception_message\": \"'total_miles_travelled'\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"success\": false,\n",
      "    \"expectation_config\": {\n",
      "      \"expectation_type\": \"expect_column_values_to_be_of_type\",\n",
      "      \"kwargs\": {\n",
      "        \"column\": \"total_earned\",\n",
      "        \"type_\": \"float64\",\n",
      "        \"result_format\": \"COMPLETE\"\n",
      "      },\n",
      "      \"meta\": {}\n",
      "    },\n",
      "    \"result\": {\n",
      "      \"element_count\": 5,\n",
      "      \"missing_count\": 0,\n",
      "      \"missing_percent\": 0.0,\n",
      "      \"unexpected_count\": 5,\n",
      "      \"unexpected_percent\": 100.0,\n",
      "      \"unexpected_percent_total\": 100.0,\n",
      "      \"unexpected_percent_nonmissing\": 100.0,\n",
      "      \"partial_unexpected_list\": [\n",
      "        \"379.75\",\n",
      "        \"259.75\",\n",
      "        \"276.25\",\n",
      "        \"286.75\",\n",
      "        \"360.25\"\n",
      "      ],\n",
      "      \"partial_unexpected_index_list\": [\n",
      "        0,\n",
      "        1,\n",
      "        2,\n",
      "        3,\n",
      "        4\n",
      "      ],\n",
      "      \"partial_unexpected_counts\": [\n",
      "        {\n",
      "          \"value\": \"259.75\",\n",
      "          \"count\": 1\n",
      "        },\n",
      "        {\n",
      "          \"value\": \"276.25\",\n",
      "          \"count\": 1\n",
      "        },\n",
      "        {\n",
      "          \"value\": \"286.75\",\n",
      "          \"count\": 1\n",
      "        },\n",
      "        {\n",
      "          \"value\": \"360.25\",\n",
      "          \"count\": 1\n",
      "        },\n",
      "        {\n",
      "          \"value\": \"379.75\",\n",
      "          \"count\": 1\n",
      "        }\n",
      "      ],\n",
      "      \"unexpected_list\": [\n",
      "        \"379.75\",\n",
      "        \"259.75\",\n",
      "        \"276.25\",\n",
      "        \"286.75\",\n",
      "        \"360.25\"\n",
      "      ],\n",
      "      \"unexpected_index_list\": [\n",
      "        0,\n",
      "        1,\n",
      "        2,\n",
      "        3,\n",
      "        4\n",
      "      ]\n",
      "    },\n",
      "    \"meta\": {},\n",
      "    \"exception_info\": {\n",
      "      \"raised_exception\": false,\n",
      "      \"exception_message\": null,\n",
      "      \"exception_traceback\": null\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    broken_july = broken_july_job.to_df(\n",
    "        validation_reference=structural_validation_reference\n",
    "    )\n",
    "except ValidationFailed as exc:\n",
    "    print(exc.validation_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you open this error message, you'll see:\n",
    "* the `total_miles_travelled` column is missing\n",
    "* because the `total_miles_travelled` column is missing, the datatype check also failed - so the previous column check is redundant\n",
    "* values in the `total_earned` column were of the wrong type, and great expectations lists the wrong values. some basic stats are also included.\n",
    "\n",
    "Great Expectations has successfully triggered a ValidationError and reported to us which values broke our expectations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Validating the distributions of featurized data\n",
    "\n",
    "The previous example was not truly Feast-esque in its design. We wouldn't make more sources and featureviews for testing new data. And, it's possible these types of errors would be caught by transformation logic prior to even getting pushed to the offline store. Lastly, we want to store featurized data in the offline store - not raw data. However, it served to show part of the power of Great Expectations.\n",
    "\n",
    "Now on to a much better demonstration of Great Expectations in combination with Feast and featurized data.\n",
    "\n",
    "I have created a \"featurized\" version of the `trip_stats` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1647134 entries, 0 to 1647133\n",
      "Data columns (total 27 columns):\n",
      " #   Column                      Non-Null Count    Dtype              \n",
      "---  ------                      --------------    -----              \n",
      " 0   taxi_id                     1647134 non-null  object             \n",
      " 1   day                         1647134 non-null  datetime64[ns, UTC]\n",
      " 2   trip_count_1_to_10          1647134 non-null  int64              \n",
      " 3   trip_count_11_to_25         1647134 non-null  int64              \n",
      " 4   trip_count_26_to_50         1647134 non-null  int64              \n",
      " 5   trip_count_51_to_100        1647134 non-null  int64              \n",
      " 6   trip_count_101_or_higher    1647134 non-null  int64              \n",
      " 7   total_miles_0_to_10         1647134 non-null  int64              \n",
      " 8   total_miles_10_to_25        1647134 non-null  int64              \n",
      " 9   total_miles_25_to_50        1647134 non-null  int64              \n",
      " 10  total_miles_50_to_100       1647134 non-null  int64              \n",
      " 11  total_miles_100_to_250      1647134 non-null  int64              \n",
      " 12  total_miles_250_to_500      1647134 non-null  int64              \n",
      " 13  total_miles_500_to_1000     1647134 non-null  int64              \n",
      " 14  total_miles_1000_or_higher  1647134 non-null  int64              \n",
      " 15  seconds_0_3600              1647134 non-null  int64              \n",
      " 16  seconds_3600_to_7200        1647134 non-null  int64              \n",
      " 17  seconds_7200_to_14400       1647134 non-null  int64              \n",
      " 18  seconds_14400_to_28800      1647134 non-null  int64              \n",
      " 19  seconds_28800_to_43200      1647134 non-null  int64              \n",
      " 20  seconds_43200_or_higher     1647134 non-null  int64              \n",
      " 21  earned_0_to_50              1647134 non-null  int64              \n",
      " 22  earned_50_to_100            1647134 non-null  int64              \n",
      " 23  earned_100_to_250           1647134 non-null  int64              \n",
      " 24  earned_250_to_500           1647134 non-null  int64              \n",
      " 25  earned_500_to_1000          1647134 non-null  int64              \n",
      " 26  earned_1000_or_higher       1647134 non-null  int64              \n",
      "dtypes: datetime64[ns, UTC](1), int64(25), object(1)\n",
      "memory usage: 339.3+ MB\n"
     ]
    }
   ],
   "source": [
    "featurized = pd.read_parquet(\"trip_stats_featurized.parquet\")\n",
    "featurized.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets add a source & feature view\n",
    "\n",
    "feature_data_source = FileSource(\n",
    "    path=\"trips_stats_featurized.parquet\",\n",
    "    name=\"feature_data\",\n",
    "    file_format=ParquetFormat(),\n",
    "    description=\"Featurized trip stat data.\",\n",
    "    owner=\"Malcolm Keyes\",\n",
    "    timestamp_field=\"day\"\n",
    ")\n",
    "\n",
    "feature_data_fv = FeatureView(\n",
    "    name=\"feature_data_fv\",\n",
    "    source= feature_data_source,\n",
    "    entities=[taxi_entity],\n",
    "    description=\"The columns of the featurized data.\",\n",
    "    owner=\"Malcolm Keyes\"\n",
    ")\n",
    "\n",
    "# Let's also make a feature service so we don't need to indicate every feature individually\n",
    "feature_data_fs = FeatureService(\n",
    "    name=\"driver_activity\",\n",
    "    features=[feature_data_fv]\n",
    ")\n",
    "\n",
    "store.apply([feature_data_source, feature_data_fv, feature_data_fs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feast's Saved Dataset / Data Quality Monitoring feature works best with a single saved dataset that can be used as a reference for other retrieval jobs. By analyzing a single saved dataset, you can generate a profile that can be applied to other data retrieval jobs, based on the characteristics of the reference saved dataset. However, this does not bide well for comparisons where you also want to adjust for the characteristics of comparison retrieval job.\n",
    "\n",
    "Nonetheless, here is an example of comparing a day of feature data against the previous 7 days, 30 days, and 90 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>taxi_id</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>trip_count_1_to_10</th>\n",
       "      <th>trip_count_11_to_25</th>\n",
       "      <th>trip_count_26_to_50</th>\n",
       "      <th>trip_count_51_to_100</th>\n",
       "      <th>trip_count_101_or_higher</th>\n",
       "      <th>total_miles_0_to_10</th>\n",
       "      <th>total_miles_10_to_25</th>\n",
       "      <th>total_miles_25_to_50</th>\n",
       "      <th>...</th>\n",
       "      <th>seconds_7200_to_14400</th>\n",
       "      <th>seconds_14400_to_28800</th>\n",
       "      <th>seconds_28800_to_43200</th>\n",
       "      <th>seconds_43200_or_higher</th>\n",
       "      <th>earned_0_to_50</th>\n",
       "      <th>earned_50_to_100</th>\n",
       "      <th>earned_100_to_250</th>\n",
       "      <th>earned_250_to_500</th>\n",
       "      <th>earned_500_to_1000</th>\n",
       "      <th>earned_1000_or_higher</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e45a4dfa53b65fb37c627b50cb466d6c7b14f87c5d991a...</td>\n",
       "      <td>2019-07-01 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f58be545baf2a44a9575194c9d6d8276e32781fae982ac...</td>\n",
       "      <td>2019-07-01 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51a62d1739477e4b43c9a98c33788451f01eedde48c0fe...</td>\n",
       "      <td>2019-07-01 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e67eb932b76835a3010b0a51a8d0624d72714fee077c59...</td>\n",
       "      <td>2019-07-01 00:00:00+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f219c2491cd2a30a788a0e4f1ce3797ef607a3ffe98ecd...</td>\n",
       "      <td>2019-07-01 00:00:00+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             taxi_id  \\\n",
       "0  e45a4dfa53b65fb37c627b50cb466d6c7b14f87c5d991a...   \n",
       "1  f58be545baf2a44a9575194c9d6d8276e32781fae982ac...   \n",
       "2  51a62d1739477e4b43c9a98c33788451f01eedde48c0fe...   \n",
       "3  e67eb932b76835a3010b0a51a8d0624d72714fee077c59...   \n",
       "4  f219c2491cd2a30a788a0e4f1ce3797ef607a3ffe98ecd...   \n",
       "\n",
       "            event_timestamp  trip_count_1_to_10  trip_count_11_to_25  \\\n",
       "0 2019-07-01 00:00:00+00:00                   1                    0   \n",
       "1 2019-07-01 00:00:00+00:00                   0                    1   \n",
       "2 2019-07-01 00:00:00+00:00                   0                    1   \n",
       "3 2019-07-01 00:00:00+00:00                   0                    1   \n",
       "4 2019-07-01 00:00:00+00:00                   1                    0   \n",
       "\n",
       "   trip_count_26_to_50  trip_count_51_to_100  trip_count_101_or_higher  \\\n",
       "0                    0                     0                         0   \n",
       "1                    0                     0                         0   \n",
       "2                    0                     0                         0   \n",
       "3                    0                     0                         0   \n",
       "4                    0                     0                         0   \n",
       "\n",
       "   total_miles_0_to_10  total_miles_10_to_25  total_miles_25_to_50  ...  \\\n",
       "0                    0                     0                     1  ...   \n",
       "1                    0                     0                     1  ...   \n",
       "2                    0                     1                     0  ...   \n",
       "3                    0                     0                     1  ...   \n",
       "4                    0                     1                     0  ...   \n",
       "\n",
       "   seconds_7200_to_14400  seconds_14400_to_28800  seconds_28800_to_43200  \\\n",
       "0                      0                       0                       0   \n",
       "1                      0                       0                       0   \n",
       "2                      0                       0                       0   \n",
       "3                      1                       0                       0   \n",
       "4                      0                       0                       0   \n",
       "\n",
       "   seconds_43200_or_higher  earned_0_to_50  earned_50_to_100  \\\n",
       "0                        0               0                 0   \n",
       "1                        0               0                 0   \n",
       "2                        0               0                 0   \n",
       "3                        0               0                 0   \n",
       "4                        0               0                 1   \n",
       "\n",
       "   earned_100_to_250  earned_250_to_500  earned_500_to_1000  \\\n",
       "0                  1                  0                   0   \n",
       "1                  1                  0                   0   \n",
       "2                  1                  0                   0   \n",
       "3                  1                  0                   0   \n",
       "4                  0                  0                   0   \n",
       "\n",
       "   earned_1000_or_higher  \n",
       "0                      0  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      0  \n",
       "4                      0  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save july 1st, 2019 as saved dataset\n",
    "\n",
    "# what should I use for entities? all entities in the dataset?\n",
    "# all for 2019 (which is what is in entities.parquet)? all in\n",
    "# the last 90 days? idk man\n",
    "\n",
    "# Ideally I wouldn't be generating these point-in-time joins at all\n",
    "# and just looking at the source feature data. I don't care about the\n",
    "# entities - I just want to look at the general distributions\n",
    "\n",
    "# I guess I'll use the included 2019 entities and see how it works\n",
    "\n",
    "taxi_ids = pyarrow.parquet.read_table(\"entities.parquet\").to_pandas()\n",
    "\n",
    "entity_df = pd.merge(taxi_ids, july_first_timestamps, how='cross')\n",
    "\n",
    "july_first_features = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=feature_data_fs\n",
    ")\n",
    "\n",
    "july_first_df = july_first_features.to_df()\n",
    "july_first_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next grab and checkout the 7, 30, and 90 day descriptions\n",
    "\n",
    "# previous 7\n",
    "last_week_of_june_timestamps = pd.DataFrame()\n",
    "last_week_of_june_timestamps[\"event_timestamp\"] = pd.date_range(\"2019-06-24\", \"2019-06-30\", freq=\"D\")\n",
    "\n",
    "entity_df = pd.merge(taxi_ids, last_week_of_june_timestamps, how='cross')\n",
    "\n",
    "last_week_of_june_features = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=feature_data_fs\n",
    ")\n",
    "\n",
    "last_week_of_june_df = last_week_of_june_features.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous 30\n",
    "june_timestamps = pd.DataFrame()\n",
    "june_timestamps[\"event_timestamp\"] = pd.date_range(\"2019-06-01\", \"2019-06-30\", freq=\"D\")\n",
    "\n",
    "entity_df = pd.merge(taxi_ids, june_timestamps, how='cross')\n",
    "\n",
    "june_features = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=feature_data_fs\n",
    ")\n",
    "\n",
    "june_df = june_features.to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previous 90\n",
    "apr_to_july_timestamps = pd.DataFrame()\n",
    "apr_to_july_timestamps[\"event_timestamp\"] = pd.date_range(\"2019-04-01\", \"2019-06-30\", freq=\"D\")\n",
    "\n",
    "entity_df = pd.merge(taxi_ids, apr_to_july_timestamps, how='cross')\n",
    "\n",
    "apr_to_july_features = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=feature_data_fs\n",
    ")\n",
    "\n",
    "apr_to_july_df = apr_to_july_features.to_df()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now that we've retrieved the data for the last 7, 30, and 90 days from July 1st, 2019, we want to see how July 1st compares to these different time scales.\n",
    "\n",
    "At first I thought this might be doable by creating a saved dataset for July 1st and creating a profile for it. Then I would have used this profile to validate the previous time periods. However, since profile parameters are based off the reference dataset, this would mean I would be looking for changes from the new day of data, when I really want to see if today has changed from the previous time periods. For example, I may want to see if the mean value of `trip_count_1_to_10` has changed by more than 10% from the mean value of the last 30 days, but with a profile based off the single day of data, I would be testing if the 30-day mean was more than 10% different than the single-day data, which is not the same.\n",
    "\n",
    "So then, I would need to create a saved dataset and profiler for every different time scale, which equates to lots of data being re-saved to s3 and then needing to be thrown out again and multiple functions that are functionally very similar, but with slightly different parameters. The multiple profilers could actually be solved easily using partial functions, but creating multiple saved datasets takes time and requires file creation and deletion.\n",
    "\n",
    "Additionally, I am not certain that we actually want to do data quality monitoring on the returned `RetrievalJobs`, as they are point in time jobs, and we really just want to look at the raw feature data. What also makes this annoying is the fact that calls to `get_historical_features` requires a set of entities, and we have no interest in the entity ids, just the general distributions of data.\n",
    "\n",
    "Lastly, even if we pursued this option, there is no way to adjust the parameters based on the size of the non-reference dataset. If we receive a day of data that is really small, then the distributions could be thrown way off, simply due to noise and overall lack of data. In effect, without being able to adjust for the size of the data, the constraints on distribution changes would need to either be too big to minimize false alarms, leading to possible missed alerts - or the constraints being too tight, and causing false alarms whenever the data size is smaller than normal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
